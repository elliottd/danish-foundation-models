{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Empowering the Danish Language in the Digital Age","text":"<p>Welcome to the Danish Foundation Models (DFM) project, a pioneering initiative in the field of machine learning and natural language processing (NLP) dedicated to the Danish language. Our mission is to develop, maintain, and provide open access to high-quality foundation models tailored for Danish, promoting innovation and inclusivity in language technologies.</p> <p>Read the paper</p> <p>You can read more about the argument for Danish Language models in our publication.</p>"},{"location":"#why-danish-foundation-models","title":"Why Danish Foundation Models?","text":""},{"location":"#bridging-the-digital-language-divide","title":"Bridging the Digital Language Divide","text":"<ul> <li>Global Gap: The rise of large language models has transformed research and technology, but smaller languages like Danish risk falling behind both in development, evaluation and application.</li> <li>Local Focus: We combat this by focusing on the Danish language, ensuring that it is well-represented in the digital landscape.</li> <li>Broad Collaboration: Our project unites public and private institutions, ensuring high data quality and practical applicability of our models.</li> </ul>"},{"location":"#our-objectives","title":"Our Objectives","text":"<ol> <li>To develop and maintain state-of-the-art language models for Danish for applications within both text and speech.</li> <li>To extensively validate foundation models for Danish in a representative set of tasks.</li> <li>To maintain a high standard of documentation of models such as model cards [Mitchell et al., 2019] and datasheets [Gebru et al., 2021].</li> <li>To open-source not only the models but also all components required for reproducibility such as pre-processing, training, and validation code.</li> </ol>"},{"location":"#open-source","title":"Open-source","text":""},{"location":"#open-source-development-with-privacy-focused-data-handling","title":"Open-source Development with Privacy-Focused Data Handling","text":"<p>In our commitment to advancing open-source development, we strongly emphasise the ethical handling of data, particularly when it involves personally sensitive information or material under copyright. This ensures that we share as much as possible while protecting privacy.</p> <p>To achieve this, our project is structured to differentiate between data that can be shared openly and that which cannot.  This demarcation is documented through detailed datasheets and training logs, hereby ensuring transparency in our processes.</p> <p>Additionally, we prioritise the security of the data during its processing and training phases. All data is stored on UCloud, a platform that upholds the recognised highest standards in information security management. This commitment to data security is exemplified by UCloud's adherence to ISO27001, a globally recognised standard, ensuring that our data handling practices meet rigorous international criteria. For more information on our security measures, please visit UCloud's security documentation.</p> <p></p>"},{"location":"#contributions","title":"Contributions","text":"<p>Besides our models DFM have led to a series of positive open-source contributions, the following table include some of these contributions:</p> Project Contribution Packages NLPDedup A deduplication library derived from DFM's deduplication code Code contributions TextDescriptives Added heuristic quality measure for texts dolma Bugfixes and addition of taggers for filtering Benchmarks ScandEval Co-contributors have significant contributions to developing NLU and NLG benchmarks for Scandinavian and Germanic languages Scandinavian Embedding Benchmark The benchmark for evaluating Scandinavian embedding has been created as a part of DFM Datasets m_arc, m_mmlu, m_hellaswag, m_truthfulqa Translated versions of English datasets intended for model evaluation for these domains"},{"location":"#improving-the-danish-language-technology-landscape","title":"Improving the Danish Language Technology Landscape","text":"<p>The Danish Foundations models collaborate with the Danish Data Science Community, Centre for Humanities Computing Aarhus, The Alexandra Institute, and Center for AI Science and Applications to promote the development of Danish language tools. We continually gather information about how to improve the Danish language technologies and how to best support the community. To this end we have created a list of missing pieces for Danish NLP and we invite any 1) to add to the list, 2) solve one of the problems or 3) upvote the problems you think are most important.</p>"},{"location":"#the-team","title":"The Team","text":"<p>From the Center for Humanities Computing at Aarhus University:</p> <ul> <li>Kenneth Enevoldsen (kenneth.enevoldsen@cas.au.dk)</li> <li>Lasse Hansen (lasse.hansen@clin.au.dk)</li> <li>Martin Bernstorff (martinbernstorff@gmail.com)</li> <li>Peter Vahlstrup (imvpbv@cc.au.dk)</li> <li>Per M\u00f8ldrup Dalum (per@cas.au.dk)</li> <li>Kristoffer Laigaard Nielbo (kln@cas.au.dk)</li> </ul> <p>From the Alexandra Institute:</p> <ul> <li>Peter Bj\u00f8rn J\u00f8rgensen (peter.jorgensen@alexandra.dk)</li> <li>Rasmus Larsen (rasmus.larsen@alexandra.dk)</li> <li>Dan Saattrup Nielsen (dan.nielsen@alexandra.dk)</li> </ul> <p>From the Center for AI Science and Applicaitons at the University of Southern Denmark:</p> <ul> <li>Peter Schneider-Kamp (petersk@imada.sdu.dk)</li> </ul> <p>From Alvenir:</p> <ul> <li>Martin Carsten Nielsen (martin@alvenir.ai)</li> <li>S\u00f8ren Vejlgaard Holm (swh@alvenir.ai)</li> </ul>"},{"location":"#join-us","title":"Join Us","text":"<p>We invite collaboration and contributions from industry professionals, researchers, and the open-source community. Together, we can advance the field of Danish NLP and create a more inclusive digital future. You can reach out to us using the following channels:</p>  - DDSC Slack Join the discussion in the \"danish-foundation-models-text\"-channel  -  GitHub Discussion Ask questions or start a discussion  - GitHub Issues Noticed a bug in the code? Please create an issue  - Using the model? If you use the model, let us know it makes it easier for us to apply for funding and justify the devopment of the project. <p>Contact us </p>"},{"location":"dcc/","title":"DCC <sub>v1</sub>","text":"<p>The DCC is a composite corpus consisting of the following subcorpora. For more information about the specific subcorpora, feel free to check out the individual datasheets.</p> Name Description Size Open Access Novel Corpus Text DAGW Danish Gigaword 1B tokens \u2713 \u2717 reddit-da Danish Reddit &lt;.1B tokens \u2713 \u2717 HopeTwitter Danish Tweets 0.48B tokens \u2717 \u2713 DaNews Danish newspapers 0.5B tokens \u2717 \u2713 Netarkivet Text Danish internet &gt;100B tokens \u2717 \u2713 Speech DaRadio Danish talk radio 140,000 hours \u2717 \u2713 DaTV Danish subtitled TV 900 hours \u2717 \u2713"},{"location":"dcc/#collaborators-and-data-owners","title":"Collaborators and Data Owners","text":"<p>Data are provided in agreement with the data owners and data collaborators. The data is generally accecible by the research collaborators, though  each data agreements has their own access restrictions and might not cover all research collaborators. Access restriction are specified on the server hosting the data in accordance with the data agreements.</p> <ul> <li>Data Owners</li> <li>Aviser / dagblade</li> <li>Danmarks Statistik</li> <li>NetArkivet</li> <li>Data Collaborators</li> <li>Det Kongelige bibliotek</li> <li>Infomedia</li> <li>Research Collaborators</li> <li>Center for humanities Computing, Aarhus Universitet</li> <li>Alexandra Institutet</li> <li>Peter Schneider-Kamp, Syddansk Universitet</li> </ul>"},{"location":"intercoder_reliability/","title":"Results from corpus tagging","text":"<p>Each user tagged 100 documents unless otherwise specified. Documents were split by newlines into text-blocks, block was rated. Text-blocks longer than 1000 characters were split into multiple blocks of 1000 characters or less.</p> <p>This tagging scheme is similar to (Kreutzer et al., 2022).</p> <p>Each block was put into one of the following categories: Each user tagged 100 documents (unless otherwise specified). Each document were tagged</p> <ul> <li><code>wrong_language</code>: Not Danish</li> <li><code>skipped</code>: Unsure of category</li> <li><code>correct_language</code>: Danish text where at least 80% of the text is reasonable.</li> <li><code>not_language</code>: Text where less than 80% of the text is reasonable. Takes priority over <code>wrong_language</code>.</li> </ul> <p>Additionally, each block was tagged for pornography (yes/no) and offensiveness (yes/no).</p>"},{"location":"intercoder_reliability/#text-proportions","title":"Text proportions","text":"<p>Kenneth (Session: test)</p> <ul> <li>Date: 2022-09-05</li> <li>Sentences tagged: 102</li> <li>Documents tagged: na</li> </ul> <p>Proportions:</p> <ul> <li>69.16% of characters is <code>correct_language</code></li> <li>25.66% of characters is <code>not_language</code></li> <li>2.74% of characters is <code>skipped</code></li> <li>2.45% of characters is <code>wrong_language</code></li> <li>0.00% of characters is porn</li> <li>0.00% of characters is offensive</li> </ul> <p>Kenneth (Session: 1)</p> <ul> <li>Date: 2022-09-06</li> <li>Sentences tagged: 292</li> <li>Documents tagged: 100</li> </ul> <p>Proportions:</p> <ul> <li>68.03% of characters is <code>correct_language</code></li> <li>29.19% of characters is <code>not_language</code></li> <li>2.10% of characters is <code>skipped</code></li> <li>0.68% of characters is <code>wrong_language</code></li> <li>0.00% of characters is porn</li> <li>1.38% of characters is offensive</li> </ul> <p>Lasse (Session: 1)</p> <ul> <li>Date: 2022-09-07</li> <li>Sentences tagged: 336</li> <li>Documents tagged: 100</li> </ul> <p>Proportions:</p> <ul> <li>68.02% of characters is <code>correct_language</code></li> <li>30.97% of characters is <code>not_language</code></li> <li>1.01% of characters is <code>wrong_language</code></li> <li>0.26% of characters is porn</li> <li>0.00% of characters is offensive</li> </ul>"},{"location":"intercoder_reliability/#intercoder-reliability","title":"Intercoder Reliability","text":"<p>Kenneth (Session: test) vs Kenneth - (Session: 1)</p> <ul> <li> <p>Cohen's Kappa (all categories): 0.8242 (Overlap in sentences: 98)</p> </li> <li> <p>Cohen's Kappa (correct_language vs not correct_language): 0.9075 (Overlap in sentences: 98)</p> </li> </ul> <p>Kenneth (Session: test) vs Lasse - (Session: 1)</p> <ul> <li> <p>Cohen's Kappa (all categories): 0.8140 (Overlap in sentences: 95)</p> </li> <li> <p>Cohen's Kappa (correct_language vs not correct_language): 0.8389 (Overlap in sentences: 95)</p> </li> </ul> <p>Kenneth (Session: 1) vs Lasse - (Session: 1)</p> <ul> <li> <p>Cohen's Kappa (all categories): 0.6767 (Overlap in sentences: 245)</p> </li> <li> <p>Cohen's Kappa (correct_language vs not correct_language): 0.7259 (Overlap in sentences: 245)</p> </li> </ul> <p>Comparison with mC4</p> <p>Note: mC4 did have a high degree of repititious texts. Similarly it did when texts blocks where not language they were often something like:</p> <pre><code>2lineStart%22%3A%22%22%2C%22placeholder%22%3A1%2C%22extName%22%3A%22nowiki%22%7D\"\" class=\"\"placeholder placeholder-ext\"\" contenteditable=\"\"false\"\"&gt;]&amp;#x200b;&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;&amp;#x200b;&lt;/span&gt;, at en lurifaks som Jimmy page, bruger MIT navn til opfindelsen! SV&lt;span data-rte-instance=\"\"1524-12953202845f3523698f3f1\"\" data-rte-meta=\"\"%7B%22type%22%3A%22ext%22%2C%22wikitext%22%3A%22%3Cref%3ESVIN%3C%5C%2Fref%3E%22%2C%22lineStart%22%3A%22%22%2C%22placeholder%22%3A1%2C%22extName%22%3A%22ref%22%7D\"\" class=\"\"placeholder placeholder-ext\"\" contenteditable=\"\"false\"\"&gt;&lt;sup data-rte-washtml=\"\"1\"\" id=\"\"cite_ref-2\"\" class=\"\"reference\"\" data-rte-attribs=\"\"\n</code></pre> <p>While non-language texts in NAT was often menu bars, contact information, or navigation.</p> <p>Kenneth (Session: 1)</p> <ul> <li>Date: 2022-09-06</li> <li>Sentences tagged: 325</li> <li>Documents tagged: 100</li> </ul> <p>Proportions:</p> <ul> <li>62.47% of characters is <code>correct_language</code></li> <li>34.88% of characters is <code>not_language</code></li> <li>1.27% of characters is <code>skipped</code></li> <li>1.38% of characters is <code>wrong_language</code></li> <li>3.25% of characters is porn</li> <li>0.00% of characters is offensive</li> </ul>"},{"location":"models/","title":"Models","text":"<p>This section gives an overview of the models available through the DFM project. The models are available through the Huggingface model hub. To avoid duplicating information surrounding the models and the information regarding the models are available at the models model sheet.</p>"},{"location":"models/#text-models","title":"Text Models","text":"Model Model type Size (parameters) munin-7b-alpha Decoder 7.24B encoder-large-v1 Encoder large (355M) encoder-medium-v1 Encoder medium (110M) encoder-small-v1 Encoder small (22M)"},{"location":"models/#speech-models","title":"Speech Models","text":"Model Model type xls-r-300m-danish Pretrained wav2vec2.0 model xls-r-300m-danish-nst-cv9 Automatic speech recognition chcaa/xls-r-300m-nst-cv9-da Automatic speech recognition"},{"location":"blog/","title":"Posts","text":""},{"location":"blog/2024/02/02/tutorial-finetuning-language-models/","title":"Tutorial: Finetuning Language Models","text":"<p>This notebook will allow you to try out finetuning of the <code>munin-7b-alpha</code> model or, indeed, any other generative model out there.</p> <p>We'll be finetuning the model on a Danish translated instruction tuning dataset, using the QLoRA method.</p>"},{"location":"blog/2024/02/02/tutorial-finetuning-language-models/#install-dependencies","title":"Install Dependencies","text":"<pre><code># Uncomment to install packages (already done for you)\n# %pip install --upgrade --force-reinstall --no-cache-dir torch==2.1.1 triton --index-url https://download.pytorch.org/whl/cu121\n# %pip install \"unsloth[cu121_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git\"\n</code></pre> <pre><code># General packages\nimport torch\nimport getpass\n\n# For loading the finetuning datasets\nfrom datasets import load_dataset\n\n# For loading and finetuning the models\nfrom unsloth import FastLanguageModel\nfrom trl import SFTTrainer, setup_chat_format\nfrom transformers import TrainingArguments, AutoTokenizer, TextStreamer, GenerationConfig\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-finetuning-language-models/#get-hugging-face-token","title":"Get Hugging Face Token","text":"<p>To allow finetuning gated models (like LLaMA-2) and to upload your finetuned models, you can put your Hugging Face token in the cell below.</p> <p>You can generate a token at https://hf.co/settings/tokens.</p> <p>If you don't want to supply a token then simply leave it blank!</p> <pre><code>HUGGING_FACE_TOKEN = getpass.getpass(\"Hugging Face Token: \")\nif not HUGGING_FACE_TOKEN:\n    print(\"Not using a Hugging Face token.\")\n    HUGGING_FACE_TOKEN = None\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-finetuning-language-models/#configure-the-model","title":"Configure the Model","text":"<pre><code>RANDOM_SEED = 42\n\nMODEL_CONFIGURATION = dict(\n    model_name=\"danish-foundation-models/munin-7b-alpha\",\n    max_seq_length=2048,  \n    dtype=None,  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+ GPUs\n    load_in_4bit=True,  # Use 4bit quantisation to reduce memory usage. Quantises on the fly, so can take a while.\n    attn_implementation=\"flash_attention_2\"\n)\n\nPEFT_CONFIGURATION = dict(\n    r = 16,  # Adapter rank, choose any number &gt; 0, but suggested 8, 16, 32, 64, 128\n    target_modules=[\n        \"q_proj\", \n        \"k_proj\", \n        \"v_proj\", \n        \"o_proj\", \n        \"gate_proj\", \n        \"up_proj\", \n        \"down_proj\",\n    ],\n    lora_alpha = 16,\n    lora_dropout = 0,  # Supports any, but = 0 is optimized\n    bias = \"none\",  # Supports any, but = \"none\" is optimized\n    use_gradient_checkpointing = True,\n    use_rslora = False,  # Support rank stabilized LoRA\n    loftq_config = None,  # And LoftQ\n    random_state = RANDOM_SEED,\n)\n\nFINETUNING_CONFIGURATION = dict(\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=1,\n    warmup_steps=5,\n    num_train_epochs=1,\n    learning_rate=2e-4,\n    weight_decay=0.01,\n    lr_scheduler_type=\"linear\",\n)\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-finetuning-language-models/#load-the-model","title":"Load the Model","text":"<pre><code>model, tokenizer = FastLanguageModel.from_pretrained(**MODEL_CONFIGURATION, token=HUGGING_FACE_TOKEN)\nmodel, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\nmodel = FastLanguageModel.get_peft_model(model, **PEFT_CONFIGURATION)\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-finetuning-language-models/#load-and-prepare-data","title":"Load and Prepare Data","text":"<p>Load the dataset from Hugging Face Hub:</p> <pre><code>dataset = load_dataset(\"kobprof/skolegpt-instruct\", split=\"train\")\nprint(f\"Number of samples in dataset: {len(dataset):,}\")\n</code></pre> <p>We just take a random subset, 1000 samples should take around 7 minutes on this machine depending on settings.</p> <pre><code>n_samples = 1000\ndataset = dataset.shuffle(seed=RANDOM_SEED).select(range(n_samples))\n</code></pre> <p>Lastly, we set up the conversations in the dataset into the standard ChatML format.</p> <pre><code>def create_conversation(sample: dict) -&gt; dict[str, list[dict[str, str]]]:\n    \"\"\"This converts the sample to the standardised ChatML format.\n\n    Args:\n        sample:\n            The data sample.\n\n    Returns:\n        The sample set up in the ChatML format.\n    \"\"\"\n    return {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": sample[\"system_prompt\"]},\n            {\"role\": \"user\", \"content\": sample[\"question\"]},\n            {\"role\": \"assistant\", \"content\": sample[\"response\"]}\n        ]\n    }\n\ndataset = dataset.map(create_conversation, batched=False)\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-finetuning-language-models/#finetune","title":"Finetune!","text":"<pre><code>trainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    max_seq_length=MODEL_CONFIGURATION[\"max_seq_length\"],\n    dataset_num_proc=4,\n    packing=True,  # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        optim=\"adamw_8bit\",\n        fp16=not torch.cuda.is_bf16_supported(),\n        bf16=torch.cuda.is_bf16_supported(),\n        logging_steps=3,\n        seed=RANDOM_SEED,\n        output_dir=\"outputs\",\n        **FINETUNING_CONFIGURATION\n    ),\n)\n</code></pre> <pre><code># Log some GPU stats before we start the finetuning\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(\n    f\"You're using the {gpu_stats.name} GPU, which has {max_memory:.2f} GB of memory \"\n    f\"in total, of which {start_gpu_memory:.2f}GB has been reserved already.\"\n)\n</code></pre> <pre><code># This is where the actual finetuning is happening\ntrainer_stats = trainer.train()\n</code></pre> <pre><code># Log some post-training GPU statistics\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory / max_memory * 100, 3)\nlora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\nprint(\n    f\"We ended up using {used_memory:.2f} GB GPU memory ({used_percentage:.2f}%), \"\n    f\"of which {used_memory_for_lora:.2f} GB ({lora_percentage:.2f}%) \"\n    \"was used for LoRa.\"\n)\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-finetuning-language-models/#try-it-out","title":"Try it Out","text":"<p>Time to try out the new finetuned model. First we need to set up how to generate text with it.</p> <p>You can leave the following config as-is, or you can experiment. Here is a list of all the different arguments.</p> <pre><code>GENERATION_CONFIG = GenerationConfig(\n    # What should be outputted\n    max_new_tokens=256, \n\n    # Controlling how the model chooses the next token to generate\n    do_sample=True, \n    temperature=0.2, \n    repetition_penalty=1.2,\n    top_k=50,\n    top_p=0.95,\n\n    #\u00a0Miscellaneous required settings\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.eos_token_id,\n    use_cache=False,  #\u00a0Required by unsloth\n)\n</code></pre> <p>Let's use <code>TextStreamer</code> for continuous inference - so you can see the generation token by token, instead of waiting the whole time!</p> <pre><code>messages = [\n    dict(\n        role=\"system\",\n        content=\"\"  # Change this to anything you want\n    ),\n    dict(\n        role=\"user\",\n        content=\"Hvad synes du om Danish Foundation Models projektet? Skriv kortfattet.\"  # And change this too\n    ),\n]\n\noutputs = model.generate(\n    input_ids=tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\"),\n    streamer=TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True),\n    generation_config=GENERATION_CONFIG,\n)\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-finetuning-language-models/#share-the-model","title":"Share the Model","text":"<p>You can share your new model to the Hugging Face Hub - this requires that you've included your Hugging Face token at the top of this notebook.</p> <pre><code># model.push_to_hub(\"your_name/qlora_model\", token=HUGGING_FACE_TOKEN)\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-finetuning-language-models/#extra-export-model-to-other-frameworks","title":"Extra: Export Model to Other Frameworks","text":""},{"location":"blog/2024/02/02/tutorial-finetuning-language-models/#saving-to-float16-for-vllm","title":"Saving to float16 for vLLM","text":"<p>The popular inference framework vLLM can take advantage of having a model available in lower precision, enabling faster inference times.</p> <p>You can uncomment the following lines if you want to save the model in 16-bit or even 4-bit precision:</p> <pre><code># Merge to 16bit\n# model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\",)\n# model.push_to_hub_merged(\"hf/model\", tokenizer, save_method=\"merged_16bit\", token=HUGGING_FACE_TOKEN)\n\n# Merge to 4bit\n# model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_4bit\",)\n# model.push_to_hub_merged(\"hf/model\", tokenizer, save_method=\"merged_4bit\", token=HUGGING_FACE_TOKEN)\n</code></pre> <p>Alternatively, you can save only the adapter weights, which are very light, but which requires the base model to be able to use it:</p> <pre><code># Just LoRA adapters\n# model.save_pretrained_merged(\"model\", tokenizer, save_method=\"lora\",)\n# model.push_to_hub_merged(\"hf/model\", tokenizer, save_method=\"lora\", token=HUGGING_FACE_TOKEN)\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-finetuning-language-models/#gguf-llamacpp-conversion","title":"GGUF / llama.cpp Conversion","text":"<p>You can also save the model in the popular <code>GGUF</code> or <code>llama.cpp</code> formats, by uncommenting any of the following:</p> <pre><code># Save to 8bit Q8_0\n# model.save_pretrained_gguf(\"model\", tokenizer)\n# model.push_to_hub_gguf(\"hf/model\", tokenizer, token=HUGGING_FACE_TOKEN)\n\n# Save to 16bit GGUF\n# model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"f16\")\n# model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method=\"f16\", token=HUGGING_FACE_TOKEN)\n\n# Save to q4_k_m GGUF\n# model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"q4_k_m\")\n# model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method=\"q4_k_m\", token=HUGGING_FACE_TOKEN)\n</code></pre> <p>Now, use the <code>model-unsloth.gguf</code> file or <code>model-unsloth-Q4_K_M.gguf</code> file in <code>llama.cpp</code> or a UI based system like <code>GPT4All</code>. You can install GPT4All by going here.</p>"},{"location":"blog/2024/02/02/tutorial-merging-language-models/","title":"Tutorial: Merging Language Models","text":"<p>Model merging is a relatively new method that allows one to combine the weights of different language models into a single model.</p> <p>In this notebook you'll get to try this out, as well as try to interact with the merged model to see the results!</p> <p>The mergekit README is good to have open for this notebook.  It has descriptions and examples for the different merge methods it supports.</p>"},{"location":"blog/2024/02/02/tutorial-merging-language-models/#install-dependencies","title":"Install Dependencies","text":"<pre><code># Uncomment to install packages (already done for you)\n# !git clone https://github.com/cg123/mergekit.git\n# %cd mergekit\n# %pip install -e .\n# %cd ..\n</code></pre> <pre><code># General packages\nimport torch\nimport shutil\nfrom pathlib import Path\n\n# For merging the models\nfrom mergekit.config import MergeConfiguration\nfrom mergekit.merge import MergeOptions, run_merge\n\n# For loading the models and running them after the merge\nimport transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, GenerationConfig\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-merging-language-models/#get-hugging-face-token","title":"Get Hugging Face Token","text":"<p>To allow merging gated models (like LLaMA-2) and to upload your merged models, you can put your Hugging Face token in the cell below.</p> <p>You can generate a token at https://hf.co/settings/tokens.</p> <p>If you don't want to supply a token then simply leave it blank!</p> <pre><code>import getpass\nHUGGING_FACE_TOKEN = getpass.getpass(\"Hugging Face Token: \")\nif not HUGGING_FACE_TOKEN:\n    print(\"Not using a Hugging Face token.\")\n    HUGGING_FACE_TOKEN = None\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-merging-language-models/#configure-the-merge","title":"Configure the Merge","text":"<p>This is where we set up which models we would like to merge, and which merging method to use.</p> <p>This configuration was the configuration used to create the Munin-NeuralBeagle model, but you can change it to whatever you like!</p> <pre><code>merge_config = dict(\n    models=[\n        dict(\n            model=\"danish-foundation-models/munin-7b-alpha\",\n        ),\n        dict(\n            model=\"mlabonne/NeuralBeagle14-7B\",\n            parameters=dict(\n                density=0.53,\n                weight=0.6,\n            ),\n        ),\n    ],\n    merge_method=\"dare_ties\",\n    base_model=\"danish-foundation-models/munin-7b-alpha\",\n    parameters=dict(\n        int8_mask=True,\n    ),\n    dtype=\"bfloat16\",\n)\n</code></pre> <pre><code>LAZY_UNPICKLE = False  # Experimental low-memory model loader\nLOW_CPU_MEMORY = True  # Enable if you have more VRAM than RAM+swap\nOUT_PATH = \"./merged\"\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-merging-language-models/#merge","title":"Merge!","text":"<pre><code>run_merge(\n    MergeConfiguration.model_validate(merge_config),\n    out_path=OUT_PATH,\n    options=MergeOptions(\n        lora_merge_cache=\"/tmp\",\n        cuda=torch.cuda.is_available(),\n        copy_tokenizer=True,\n        lazy_unpickle=LAZY_UNPICKLE,\n        low_cpu_memory=LOW_CPU_MEMORY,\n    )\n)\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-merging-language-models/#try-it-out","title":"Try it Out","text":"<p>Time to try out the new merged model. Let's start by loading it from disk.</p> <pre><code>model = AutoModelForCausalLM.from_pretrained(OUT_PATH, load_in_4bit=True)\ntokenizer = AutoTokenizer.from_pretrained(OUT_PATH)\n\n# Choosing a chat template for a merged model can be difficult. The one defined in \n# NeuralBeagle seems broken. Additionally, it does not have special tokens that some \n# of the merged models might have been trained with\ntokenizer.chat_template = \"\"\"\n{% if not add_generation_prompt is defined %}\n    {% set add_generation_prompt = false %}\n{% endif %}\n{% for message in messages %}\n    {{'&lt;|im_start|&gt;' + message['role'] + '\\n' + message['content'] + '&lt;|im_end|&gt;' + '\\n'}}\n{% endfor %}\n{% if add_generation_prompt %}\n    {{ '&lt;|im_start|&gt;assistant\\n' }}\n{% endif %}\n\"\"\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    device_map=\"auto\",\n)\n</code></pre> <p>Next, we need to set up how to generate text with it. You can leave the following config as-is, or you can experiment. Here is a list of all the different arguments.</p> <pre><code>GENERATION_CONFIG = GenerationConfig(\n    # What should be outputted\n    max_new_tokens=256, \n\n    # Controlling how the model chooses the next token to generate\n    do_sample=True, \n    temperature=0.2, \n    repetition_penalty=1.2,\n    top_k=50,\n    top_p=0.95,\n\n    #\u00a0Miscellaneous required settings\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.eos_token_id\n)\n</code></pre> <pre><code>messages = [\n    dict(\n        role=\"system\",\n        content=\"\"  # Change this to anything you want\n    ),\n    dict(\n        role=\"user\",\n        content=\"Hvad er en stor sprogmodel?\"  # And change this too\n    ),\n]\n\noutputs = pipeline(\n    tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True), \n    streamer=TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True),\n    generation_config=GENERATION_CONFIG,\n)\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-merging-language-models/#share-the-model","title":"Share the Model","text":"<p>You can share your new model to the Hugging Face Hub - this requires that you've included your Hugging Face token at the top of this notebook.</p> <pre><code># model.push_to_hub(\"your_name/merged_model\", token=HUGGING_FACE_TOKEN)\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-merging-language-models/#clean-up","title":"Clean Up","text":"<p>This deletes the merged model, as well as clearing the Hugging Face cache.</p> <p>WARNING: You will have to redownload any used models if you do this!</p> <pre><code># shutil.rmtree(OUT_PATH, ignore_errors=True)\n# shutil.rmtree('/home/ubuntu/.cache', ignore_errors=True)\n</code></pre>"},{"location":"blog/2024/01/11/releasing-munin-7b-alpha---a-danish-llm/","title":"Releasing Munin 7B Alpha - A Danish LLM","text":"<p>We are excited to announce the release of the first model from the Danish Foundation Models project, nicknamed Munin 7B Alpha. This model represents the beginning of our research into Danish Large Language Models (LLMs), employing continual pre-training based on the already pre-trained Mistral-7b-v0.1 model. It has been pre-trained on the Danish Gigaword dataset, which has been instrumental in training various Danish BERT-style models.</p> <p>The model has been trained for one epoch over the dataset and ends up with a loss 1.27 on the Danish Gigaword. See more the model training logs here.</p> <p>This release underscores our commitment to transparency about our work and the challenges we are facing. We want to clearly note that we expect the model to perform suboptimally for many, if not most, applications. Our evaluations on the limited generative Danish tasks available to us have indicated that our current training approach may negatively impact performance on these downstream tasks, even compared to the upstream Mistral model.</p> Model Name Overall Score Danish Score Norwegian Score Swedish Score gpt-3.5-turbo-0613 58.52 \u00b1 2.42 56.72 \u00b1 2.44 57.31 \u00b1 2.37 61.54 \u00b1 2.46 mistralai/Mistral-7B-v0.1 40.30 \u00b1 2.15 39.60 \u00b1 1.94 35.98 \u00b1 2.54 45.31 \u00b1 1.96 danish-foundation-models/munin-7b-alpha 37.50 \u00b1 2.49 39.56 \u00b1 2.70 30.82 \u00b1 2.69 42.13 \u00b1 2.07 AI-Sweden-Models/gpt-sw3-6.7b-v2 26.67 \u00b1 2.30 23.65 \u00b1 2.02 24.28 \u00b1 2.74 32.08 \u00b1 2.13 mhenrichsen/danskgpt-tiny 16.87 \u00b1 3.05 16.66 \u00b1 2.18 15.16 \u00b1 2.64 18.80 \u00b1 4.35 <p>See the full ScandEval leaderboard for an up-to-date comparison. Despite these challenges, we hope that our open approach encourages the community to collaborate with us in building the best possible Danish LLM. While the current version of the model may not yet be a practical tool for Danish NLP, we believe that sharing our findings is valuable. A critical need has been identified: access to a significantly larger corpus of Danish text data, and a legal framework that reliably allows for training and releasing open models, including for commercial use.</p> <p>At Danish Foundation Models, we are actively pursuing legal access to extensive Danish text data, and are exploring every option for releasing models under the most open license possible. We have already secured agreements that provide us access to several large Danish datasets, and we plan to include these into our training process in the near future.</p> <p>In summary, Munin 7B Alpha is a small step forward. It signifies our commitment to advancing Danish NLP and acknowledges the extensive work ahead. By sharing this model, we aim to foster collaborative efforts within the community. The model is now available for download and experimentation, and we look forward to your insights and discussions on how we can progress.</p> <p>The development of this model, and the Danish Foundation Models project in general, is generously supported by the following:</p> <ul> <li>Danish e-infrastructure Consortium</li> <li>Acquisition and Logistics Organisation at the Danish Ministry of Defence</li> <li>Danish Ministry of Higher Education and Science under the Digital Security, Trust   and Data Ethics performance contract</li> </ul>"},{"location":"datasheets/danews/","title":"DaNews","text":"<p>Version: 1.0.0</p> <p>Homepage: https://github.com/centre-for-humanities-computing/danish-foundation-models</p> <p>license: Not publicly available.</p> <p>DaNews consists of articles from Danish news and tabloid media from 1 December 2019 to  30 April 2021. The articles stem from multiple news sources, including both online of physical newspapers.</p> <p>DaNews consists of 403 million tokens 93% were left after  quality filtering and deduplication.</p>"},{"location":"datasheets/danews/#datasheet","title":"Datasheet","text":"<p>Following the recommendation and framework of [5] we add the following datasheet. </p>"},{"location":"datasheets/danews/#motivation","title":"Motivation","text":"<p>For what purpose was the dataset created? Who created the dataset? Who funded the creation of the dataset?</p> <p>DANews was collected as a part of the HOPE project, examining news coverage during the COVID-19 pandemic. The purpose was to train a model to understand how the novelty and resonance imprint of COVID-19 as a case of crisis compared to non-crises news imprints.</p> <p>Any other comments?</p> <p>No.</p>"},{"location":"datasheets/danews/#composition","title":"Composition","text":"<p>How many instances are there in total (of each type, if appropriate)?</p> <p>The unfiltered dataset consists of 713 429 documents including a total of 403 089 625 tokens.</p> <p>What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?</p> <p>Instances of the dataset are Danish articles derived from Danish tabloids or news media. </p> <p>Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?</p> <p>Prior to filtering DaNews dataset contains all digitized news articles from the given period across the sources.</p> <p>What data does each instance consist of? \u201cRaw\u201d data (e.g., unprocessed text or images) or features? In either case, please provide a description.</p> <p>Each instance consists of the following columns <pre><code>'ArticleUrl', 'Heading', 'SubHeading', 'Lead', 'Paragraph', 'PublishDate', 'BodyText', \n'Captions', 'Authors', 'Source', 'WordCount', 'ArticleId', 'PageIds', 'Section', 'text'\n</code></pre></p> <p>Where we constructed the columns <code>text</code> column by joining the <code>Heading</code>, <code>SubHeading</code> using newline. If the text field is empty it is ignored and no newline is added. The we join the resulting string with the <code>BodyText</code> using two newlines.</p> <p>During the quality filtering, we add the following indicator columns: <pre><code>'passed_quality_filter', 'filtered_by_max_chr_length', 'filtered_by_doc_length', \n'filtered_by_mean_word_length', 'filtered_by_alpha_ratio', 'filtered_by_stop_word', \n'filtered_by_symbol_2_word_hashtag', 'filtered_by_symbol_2_word_ellipsis',\n'filtered_by_line_bullets_or_ellipsis', 'filtered_by_duplicate_lines_chr_fraction',\n'filtered_by_duplicate_paragraph_chr_fraction', 'filtered_by_top_ngram_chr_fraction',\n'filtered_by_duplicate_ngram_chr_fraction', 'is_duplicate'\n</code></pre></p> <p>Is there a label or target associated with each instance? If so, please provide a description.</p> <p>No.</p> <p>Is any information missing from individual instances? If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable). This does not include intentionally removed information but might include, e.g., redacted text.</p> <p>The team of researchers at the Humanities Computing Aarhus (CHCAA) have not removed any information from the instances.</p> <p>Are relationships between individual instances made explicit (e.g., users\u2019 movie ratings, and social network links)? If so, please describe how these relationships are made explicit.</p> <p>The metadata columns denote the relationship between articles including the date of publication, sections, and authors.</p> <p>Are there recommended data splits (e.g., training, development/validation, testing)? If so, please provide a description of these splits, explaining the rationale behind them.</p> <p>There are not splits performed on this dataset.</p> <p>Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description.</p> <p>News sources can publish their content both in an online and printed format which would lead to similar instances in the dataset. To alleviate this redundancy by removing near-duplicates (see Preprocessing/cleaning/labeling).</p> <p>Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)?</p> <p>Articles are intended to tell a self-contained story but can include external references such as tweets or website URLs.</p> <p>Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?</p> <p>Articles often describe content that is considered offensive, insulting, or threatening. </p>"},{"location":"datasheets/danews/#collection-process","title":"Collection Process","text":"<p>What mechanisms or procedures were used to collect the data (e.g., hardware  apparatuses or sensors, manual human curation, software programs, software APIs)?</p> <p>A team of researchers at the Center for Humanities Computing Aarhus (CHCAA) obtained this  dataset using a third-party API as well as a manual transfer from one of the parties. The API was limited  to only a subset of articles agreed upon within the agreements.</p> <p>If the dataset is a sample from a larger set, what was the sampling strategy?</p> <p>The dataset is not a sample, but is a filtered version of the full dataset, see Preprocessing/cleaning/labeling for more on this.</p> <p>Who was involved in the data collection process? A team of researchers at the Center for Humanities Computing Aarhus (CHCAA) obtained this dataset using a third party API as well as a manual transfer from some of the parties and would like to thank the dataset owners for  access to their articles.</p> <p>Over what timeframe was the data collected?</p> <p>The dataset includes articles from 1 December 2019 to  30 April 2021.</p> <p>Were any ethical review processes conducted?</p> <p>No.</p>"},{"location":"datasheets/danews/#preprocessingcleaninglabeling","title":"Preprocessing/cleaning/labeling","text":"<p>Was any preprocessing/Cleaning/Labeling of the data done  (e.g., discretization or bucketing, tokenization, part-of-speech tagging,  SIFT feature extraction, removal of instances, processing of missing values)?</p> <p>DaNews has been filtered using a series of heuristic filters as well as removing repetitious texts. Following the filtering, DaNews is deduplicated to remove exact and near-duplicates.</p> <p>Of all documents, 9% were filtered based due to low-quality and 4% because they were near-duplicates.</p> <p>For quality filtering, DaNews applies a filter akin to [2] which contains text that:</p> <ul> <li>Contain at least 2 Danish stopwords. For the stopword list we use the one used in SpaCy v.3.1.4.</li> <li>Have a mean word length between 3 and 10.</li> <li>Have a token length between 50 and 100,000.</li> <li>Have less than 5,000,000 characters.</li> <li>Have less than 60% of words containing an alphabetic character.</li> <li>Have a symbol-to-word ratio lower than 10% for hashtags and ellipsis.</li> <li>Have less than 90% of lines starting with a bullet point.</li> <li> <p>have less than 30% of lines ending with an ellipsis.</p> </li> <li> <p>Have a low high degree of repetitious text:</p> </li> <li>Have less than 20% of characters contained within duplicate lines.</li> <li>Have less than 20% of characters contained within duplicate paragraphs.</li> <li>Where the top 2-4 grams constitute less than 20%, 18%, 16%, respectively, of the text. </li> <li>Where the duplicate 5-10 grams constitute less than 25%, 24%, 23%, 22%, 21%, 20% of the text, respectively.</li> </ul> <p>The deduplication removed all documents with a 13-gram Jaccard similarity higher than 80% following the MinHash algorithm [1] using 128 permutations. The MinHash algorithm is a probabilistic data structure for approximating the Jaccard similarity between two sets.</p> <p>Is the software used to preprocess/clean/label the instances available?</p> <p>Yes, the scripts are available here.  the scripts use version 0.0.2 of the dfm package. </p>"},{"location":"datasheets/danews/#uses","title":"Uses","text":"<p>Has the dataset been used for any tasks already?</p> <p>Yes, the dataset has been used to pre-train Danish language models. Parts of the dataset have also been used in [3] and [4]</p> <p>Is there a repository that links to any or all papers or systems that use the dataset?</p> <p>No.</p> <p>What (other) tasks could the dataset be used for?</p> <p>The scale of the dataset makes it suitable for NLP tasks such as language modeling. Similarly, the structure of the articles makes it a suitable dataset for training text summarisation models.</p> <p>Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?</p> <p>This dataset is static and thus does not evolve over time with the language.  A consequence of this is that it will become increasingly outdated over time.</p> <p>Are there tasks for which the dataset should not be used?</p> <p>This dataset contains Danish articles and thus should not be used for non-Danish language tasks.</p> <p>As the writers of the content are predominantly journalists, it reflects a certain writing style which is unlikely to reflect the Danish language as a whole.</p>"},{"location":"datasheets/danews/#distribution","title":"Distribution","text":"<p>Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created?</p> <p>Data will only be available at the entity during the project. If you wish access to the dataset you will have to come to an agreement with the individuals Danish newspapers.</p>"},{"location":"datasheets/danews/#citation","title":"Citation","text":"<p>If you wish to cite this work please see our GitHub page for an up-to-date citation: https://github.com/centre-for-humanities-computing/danish-foundation-models</p>"},{"location":"datasheets/danews/#references","title":"References:","text":"<ul> <li>[1] Broder, Andrei Z. \"On the resemblance and containment of documents.\"         Proceedings. Compression and Complexity of SEQUENCES 1997         (Cat. No. 97TB100171). IEEE, 1997.</li> <li>[2] Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F.,         Aslanides, J., Henderson, S., Ring, R., Young, S., Rutherford, E., Hennigan,         T., Menick, J., Cassirer, A., Powell, R., Driessche, G. van den, Hendricks,         L. A., Rauh, M., Huang, P.-S., \u2026 Irving, G. (2021).         Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher.         https://arxiv.org/abs/2112.11446v2</li> <li>[3] Baglini, R. B., Nielbo, K. L., H\u00e6strup, F., Enevoldsen, K., Vahlstrup, P. B., &amp;          Roepstorff, A. (2021, June 2). When no news is bad news: Detection of negative         events from news media content. https://2021.dhbenelux.org/</li> <li>[4] Nielbo, K. L., Baglini, R. B., Vahlstrup, P. B., Enevoldsen, K. C., Bechmann, A.,         &amp; Roepstorff, A. (2021, January). News information decoupling: An information         signature of catastrophes in legacy news media. https://eadh2020-2021.org/</li> <li>[5] T. Gebru, J. Morgenstern, B. Vecchione, J. W. Vaughan, H. Wallach, H. Daum\u00e9 III,         and K. Crawford. Datasheets for datasets. arXiv preprint arXiv:1803.09010, 2018.</li> </ul>"},{"location":"datasheets/daradio/","title":"DaRadio Datasheet","text":"<p>Version: 1.0.0</p> <p>Homepage: https://github.com/centre-for-humanities-computing/danish-foundation-models</p> <p>License: Not publicly available.</p> <p>DaRadio consists of radio broadcasts from the Danish radio stations DR P1 and Radio24Syv, and contains approximately 140.000 hours of speech. DaRadio includes all shows aired on DR P1 from 2005 to 2021, and all shows aired on Radio24Syv from 2011 to 2019.</p> <p>DaRadio has been deduplicated using a series of heuristics based on metadata. For more on deduplication, see the data cleaning section further below.</p>"},{"location":"datasheets/daradio/#datasheet","title":"Datasheet","text":"<p>Following the recommendation and framework of [1], we add the following datasheet. </p>"},{"location":"datasheets/daradio/#motivation","title":"Motivation:","text":"<p>For what purpose was the dataset created? Who created the dataset? Who funded the creation of the dataset?</p> <p>Data included in DaRadio was collected following the Danish Legal Deposit Act by the Royal Danish Library (RDL). From this, a dataset of Danish speech-only radio was derived by RDL. The dataset was created for research purposes, including training a Danish wav2vec2.0 model. </p> <p>The dataset was preprocessed to remove duplicates by a team of researchers at the Center for Humanities Computing, Aarhus University (CHC) with collaborators from the Danish speech-processing company Alvenir.</p>"},{"location":"datasheets/daradio/#composition","title":"Composition","text":"<p>What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?</p> <p>Instances of the dataset include an mp3 file for each show aired on the two staions within the period. Further metadata include information on date and time of airing, title, short description of the show, and various internal identifiers used by RDL.</p> <p>How many instances are there in total (of each type, if appropriate)?</p> <p>DaRadio consists of a total of 215.582 hours of unprocessed Danish speech radio shows across two stations, DR P1 and Radio24syv. The table below shows the distribution over the stations with and without heuristic rerun removal.</p> Source Duration (hours) Reruns removed P1 145.160 False 97.401 True Radio24syv 70.422 False 44.569 True <p>Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?</p> <p>The dataset contains all shows from the two stations in the time period (2005-2021 for DR P1 and 2011-2019 for Radio24syv).</p> <p>If the dataset is a sample from a larger set, what was the sampling strategy?</p> <p>The dataset is a subset of all Danish radio. The two stations were chosen for the dataset as they are talk-radio only. </p> <p>Who was involved in the data collection process?</p> <p>The RDL collects Danish radio shows and constructed DaRadio for handing to researchers at CHC.</p> <p>Over what timeframe was the data collected?</p> <p>The dataset includes radio shows from the period 2005 to 2021.</p> <p>Were any ethical review processes conducted?</p> <p>The RDL collects radio shows in adherence to Danish Archival laws. DaRadio was constructed for a research project, for which a project proposal was accepted by RDL. No other ethical review processes were conducted.</p>"},{"location":"datasheets/daradio/#preprocessingcleaninglabeling","title":"Preprocessing/cleaning/labeling","text":"<p>Was any preprocessing/Cleaning/Labeling of the data done  (e.g., discretization or bucketing, tokenization, part-of-speech tagging,  SIFT feature extraction, removal of instances, processing of missing values)?</p> <p>DaRadio has been deduplicated using a series of heuristic filters and all files have been converted to 16 Khz .wav files. </p> <p>Reruns/duplicates were identified by the following rules:</p> <ul> <li>If the phrase \"sendt f\u00f8rste gang\" [\"aired the first time\"] or \"genudsendelse\" [\"rerun\"] appeared in the show description.</li> <li>If the title contained \"(G)\" (short for \"genudsendelse\")) </li> <li>If the show was broadcast between 23:00 and 5:00.</li> </ul> <p>The deduplication was coded and conducted by researchers at CHC.</p> <p>Is the software used to preprocess/clean/label the instances available?</p> <p>The scripts are available at the following GitHub repository: link.</p>"},{"location":"datasheets/daradio/#uses","title":"Uses","text":"<p>Has the dataset been used for any tasks already?</p> <p>Yes, the dataset has been used to pre-train a Danish wav2vec2.0 model. </p> <p>Is there a repository that links to any or all papers or systems that use the dataset?</p> <p>No, but as of 23/10/16 no others have used the dataset.</p> <p>What (other) tasks could the dataset be used for?</p> <p>As the dataset only contains un-labelled data, i.e. no transcriptions, it is mainly designed for pre-training language models. However, given the metadata and re-occuring hosts, further processing might make it possible to train e.g. text-to-speech systems. </p> <p>Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?</p> <p>This dataset is static and does not evolve over time with the language, thus will become increasingly outdated over time.</p>"},{"location":"datasheets/daradio/#distribution","title":"Distribution","text":"<p>Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created?</p> <p>Data will only be available at the entity during the project. An equivalent or updated dataset can be requested at the Royal Danish Library.</p>"},{"location":"datasheets/daradio/#citation","title":"Citation","text":"<p>If you wish to cite this work please see our GitHub page for an up to date citation: https://github.com/centre-for-humanities-computing/danish-foundation-models</p>"},{"location":"datasheets/daradio/#references","title":"References:","text":"<ul> <li>[1] T. Gebru, J. Morgenstern, B. Vecchione, J. W. Vaughan, H. Wallach, H. Daum\u00e9 III,         and K. Crawford. Datasheets for datasets. arXiv preprint arXiv:1803.09010, 2018.</li> </ul>"},{"location":"datasheets/hopetwitter/","title":"HopeTwitter","text":"<p>Version: 1.0.0</p> <p>Homepage: https://github.com/centre-for-humanities-computing/danish-foundation-models</p> <p>license: Not publicly available.</p> <p>HopeTwitter consists of tweets collected from the Twitter API using a stopword list and consists of 32.5 million tweets across 538,398 unique users. HopeTwitter includes tweets from 2019-01-01 to 2021-04-30.</p> <p>HopeTwitter, have been filtered to only include Danish tweets, based on language tag from Twitter API. Similarly, HopeTwitter have had low-quality tweets have removed and then deduplicated to remove exact and near-duplicates. For more on data cleaning see section; \"Preprocessing/cleaning/labeling\".</p> <p>HopeTwitter includes a total of 0.97 billion tokens before filtering and includes 0.48 billion (50%) after.</p>"},{"location":"datasheets/hopetwitter/#datasheet","title":"Datasheet","text":"<p>Following the recommendation and framework of [3] we add the following datasheet. </p>"},{"location":"datasheets/hopetwitter/#motivation","title":"Motivation","text":"<p>**For what purpose was the dataset created? Who created the dataset? Who funded the  creation of the dataset? **</p> <p>HopeTwitter was initially collected as a part of the HOPE project, examining societal behaviour during the COVID-19 pandemic. Next, HopeTwitter was cleaned in preparation for pre-training Danish language models by a team of researchers at Center for Humanities Computing Aarhus  (CHCAA), using a codebase jointly developed with partners from academia and industry, including KMD, Ekstra Bladet, Bristol University and Deepdivr. For more on collaborators on this project see the GitHub repository.</p> <p>Any other comments?</p> <p>No.</p>"},{"location":"datasheets/hopetwitter/#composition","title":"Composition","text":"<p>What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?</p> <p>HopeTwitter consists of tweets containing at least one of a series of stopwords, collected through the Twitter API. See \"If the dataset is a sample from a larger set, what was the sampling strategy?\" for the stopword list.</p> <p>How many instances are there in total (of each type, if appropriate)?</p> <p>The dataset consist of 32,499,019 documents where 14,399,284 (44%) were considered duplicates. </p> <p>Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?</p> <p>No. It does not contain all instances of Danish Twitter as there are likely some Danish tweets which does not include a stopword.</p> <p>Is there a label or target associated with each instance? If so, please provide a description.</p> <p>No.</p> <p>Are there recommended data splits (e.g., training, development/validation, testing)? If so, please provide a description of these splits, explaining the rationale behind them.</p> <p>No splits are performed on this dataset.</p> <p>If the dataset is a sample from a larger set, what was the sampling strategy?</p> <p>Tweets are streamed continuously using queried a set of the highest  frequency Scandinavian-specific keywords from Danish, Norwegian (Bokm\u00e5l) and Swedish, resulting in the following list: <pre><code>aften, aldrig, alltid, altid, andet, arbejde, bedste, beh\u00f6ver, beh\u00f8ver, beklager,\nber\u00e4tta, betyr, blev, blevet, blir, blitt, blive, bliver, bruge, burde, b\u00e4ttre, b\u00e5e\nb\u00f8r, deim, deires, ditt, drar, drepe, dykk, dykkar, d\u00e4r, d\u00f6d, d\u00f6da, d\u00f8d, d\u00f8de, efter,\nelsker, endnu, faen, fandt, feil, fikk, finner, flere, forst\u00e5r, fortelle, fortfarande,\nfortsatt, fort\u00e6lle, fr\u00e5n, f\u00e5, f\u00e5et, f\u00e5r, f\u00e5tt, f\u00f6rl\u00e5t, f\u00f6rsta, f\u00f6rs\u00f6ker, f\u00f8r, f\u00f8rst,\nf\u00f8rste, gick, gikk, gillar, gjennom, gjerne, gjorde, gjort, gj\u00f8r, gj\u00f8re, godt, g\u00e5, g\u00e5ng,\ng\u00e5r, g\u00f6ra, g\u00f8r, g\u00f8re, hadde, hall\u00e5, havde, hedder, helt, helvete, hende, hendes, hennes,\nherregud, hjelp, hjelpe, hjem, hj\u00e4lp, hj\u00e5, hj\u00e6lp, hj\u00e6lpe, honom, hossen, hvem, hvis,\nhvordan, hvorfor, h\u00e4nder, h\u00e4r, h\u00e5ll, h\u00e5ller, h\u00f8r, h\u00f8re, h\u00f8rer, igjen, ikkje, ingenting,\ninkje, inte, intet, jeres, j\u00e4vla, kanske, kanskje, kender, kjenner, korleis, kvarhelst,\nkveld, kven, kvifor, k\u00e4nner, ledsen, lenger, lidt, livet, l\u00e4ngre, l\u00e5t, l\u00e5ter, l\u00e6nge,\nmeget, menar, mycket, mykje, m\u00e5, m\u00e5de, m\u00e5nga, m\u00e5r, m\u00e5ske, m\u00e5ste, m\u00e5tte, navn, nogen,\nnoget, nogle, noko, nokon, nokor, nokre, n\u00e5gon, n\u00e5got, n\u00e5gra, n\u00e5n, n\u00e5r, n\u00e5t, n\u00f8dt,\nocks\u00e5, ogs\u00e5, pengar, penger, pratar, pr\u00f8ver, p\u00e5, redan, rundt, r\u00e4tt, sagde, saker,\nsamma, sammen, selv, selvf\u00f8lgelig, sidan, sidste, siger, sikker, sikkert, sj\u00e4lv, skete,\nskjedde, skjer, skulle, sluta, slutt, snakke, snakker, snill, sn\u00e4lla, somt, stadig,\nstanna, sted, st\u00e5r, synes, s\u00e4ger, s\u00e4tt, s\u00e5, s\u00e5dan, s\u00e5g, s\u00e5nn, tager, tiden, tilbage,\ntilbake, tillbaka, titta, trenger, trodde, troede, tror, tv\u00e5, tycker, t\u00e4nker, uden,\nundskyld, unnskyld, urs\u00e4kta, uten, varf\u00f6r, varit, varte, veldig, venner, verkligen,\nvidste, vilken, virkelig, visste, v\u00e4g, v\u00e4l, v\u00e4ldigt, v\u00e4n, v\u00e5r, v\u00e5ra, v\u00e5re, v\u00e6k, v\u00e6r, \nv\u00e6re, v\u00e6ret, \u00e4lskar, \u00e5h, \u00e5r, \u00e5t, \u00f6ver\n</code></pre></p> <p>Who was involved in the data collection process?</p> <p>A team of researchers at the Center for Humanities Computing Aarhus (CHCAA), including Kristoffer Nielbo and Peter Bjerregaard Vahlstrup, in collaboration with Rebekah Baglini, at the School of Communcation and Culture at Aarhus university.</p> <p>Over what timeframe was the data collected?</p> <p>The dataset includes tweets from the period 2019-01-01 to 2021-04-30.</p> <p>Were any ethical review processes conducted?</p> <p>No</p>"},{"location":"datasheets/hopetwitter/#preprocessingcleaninglabeling","title":"Preprocessing/cleaning/labeling","text":"<p>Was any preprocessing/Cleaning/Labeling of the data done  (e.g., discretization or bucketing, tokenization, part-of-speech tagging,  SIFT feature extraction, removal of instances, processing of missing values)?</p> <p>Firstly, HopeTwitter had non-Danish tweets removed, after which a series of heuristic filters were applied, including the removal of repetitious texts. Following the filtering, HopeTwitter was deduplicated, removing both exact duplicates and near-duplicates.</p> <p>Of all documents, 3,023,427 (9%) were filtered due to low-quality and 14,399,284 (33%) because they were near-duplicates.</p> <p>For the quality filtering, HopeTwitter applies a filter akin to [2] which contains text that:</p> <ul> <li>Contain at least 2 Danish stopwords. For the stopword list we use the one used in SpaCy v.3.1.4.</li> <li>Have a mean word length between 2 and 14.</li> <li>Have a token length between 10 and 100,000.</li> <li>Have less than 5,000,000 characters.</li> <li> <p>Have less than 60% of words containing an alphabetic character.</p> </li> <li> <p>Have low high degree of repetitious text:</p> </li> <li>Have less than 20% of characters contained within duplicate lines.</li> <li>Have less than 20% of characters contained within duplicate paragraphs.</li> <li>Where the top 2-4 grams constitute less than 20%, 18%, 16%, respectively, of the text. </li> <li>Where the duplicate 5-10 grams constitute less than 25%, 24%, 23%, 22%, 21%, 20% of the text, respectively.</li> </ul> <p>The deduplication removed all documents with a 10-gram Jaccard similarity higher than 80% following the MinHash algorithm [1] using 128 permutations. The MinHash algorithm is a probabilistic data structure for approximating the Jaccard similarity between two sets.</p> <p>Is the software used to preprocess/clean/label the instances available?</p> <p>Yes, the scripts are available here. The scripts use version 0.0.2 of the dfm package. </p>"},{"location":"datasheets/hopetwitter/#uses","title":"Uses","text":"<p>Has the dataset been used for any tasks already?</p> <p>Yes, the dataset has been used to pre-train Danish language models. Parts of the dataset have also been used in HOPE project reports  and in [4].</p> <p>Is there a repository that links to any or all papers or systems that use the dataset?</p> <p>There is a website for the HOPE project for which the dataset was initially collected. This website contains report and articles regarding the dataset.</p> <p>What (other) tasks could the dataset be used for?</p> <p>The scale of the dataset makes it suitable for NLP tasks such as language modelling. Similarly, one could imagine using the conversation structure could be used to train conversational chatbots.</p> <p>Is there anything about the composition of the dataset or the way it was collected and  preprocessed/cleaned/labeled that might impact future uses?</p> <p>This dataset is static and thus does not evolve over time with the language.  A consequence of this is that it will become increasingly outdated over time. However, it possible to extend the dataset by a continual collection of tweets.</p> <p>Are there tasks for which the dataset should not be used?</p> <p>HopeTwitter contains Danish tweets and thus should not be used for non-Danish language tasks.</p> <p>As the writers of the content is predominantly journalists, politicians, influencers, and academics, it reflects a certain social group which is unlikely to reflect Danish population as a whole.</p>"},{"location":"datasheets/hopetwitter/#distribution","title":"Distribution","text":"<p>Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created?</p> <p>Data will only be available at the entity during the project. After the project the data will be archived for a period of five years to comply with the university [policy] for research integrity. After the five years, the data will be registered at the national archives as required by executive order 514 for potential long-term deposit.</p>"},{"location":"datasheets/hopetwitter/#citation","title":"Citation","text":"<p>If you wish to cite this work please see our GitHub page for an up to date citation: https://github.com/centre-for-humanities-computing/danish-foundation-models</p>"},{"location":"datasheets/hopetwitter/#references","title":"References:","text":"<ul> <li>[1] Broder, Andrei Z. \"On the resemblance and containment of documents.\"     Proceedings. Compression and Complexity of SEQUENCES 1997     (Cat. No. 97TB100171). IEEE, 1997.</li> <li>[2] Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F.,      Aslanides, J., Henderson, S., Ring, R., Young, S., Rutherford, E., Hennigan,      T., Menick, J., Cassirer, A., Powell, R., Driessche, G. van den, Hendricks,      L. A., Rauh, M., Huang, P.-S., \u2026 Irving, G. (2021).     Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher.     https://arxiv.org/abs/2112.11446v2</li> <li>[3] T. Gebru, J. Morgenstern, B. Vecchione, J. W. Vaughan, H. Wallach, H. Daum\u00e9 III,     and K. Crawford. Datasheets for datasets. arXiv preprint arXiv:1803.09010, 2018.</li> <li>[4]\u00a0Johansen, N., Marjanovic, S. V., Kjaer, C. V., Baglini, R. B., &amp; Adler-Nissen, R.      (2022). Ridiculing the \u201ctinfoil hats:\u201d Citizen responses to COVID-19 misinformation     in the Danish facemask debate on Twitter. Harvard Kennedy School Misinformation     Review. https://doi.org/10.37016/mr-2020-93</li> </ul>"},{"location":"datasheets/netarkivet_text/","title":"NAT: Netarkivet Text","text":"<p>Version: 1.0.0</p> <p>Homepage: https://github.com/centre-for-humanities-computing/danish-foundation-models</p> <p>license: Not publicly available.</p> <p>This datasheet is currently being revised \ud83d\udee0\ufe0f</p>"},{"location":"taggers_stats/taggers_speed/","title":"Statistics on Dolma and Scandi taggers","text":"<p>The dataset for testing is a subset of DAGW, which is only consist of documents from domain <code>Wiki &amp; Books</code> (430837 in total). However, during testing, 161915 documents that only contains white space(s) were found out due to the <code>ZeroDivisionErro</code> in tagger <code>pii_regex_v1</code> when there is no empty documents.</p> <p>Finally, the test on taggers performed on cleaned <code>Wiki &amp; Books</code> section of DAGW (268922 in total), each following tagger was tested only once using one proccess.</p> <p>Here is an example of how to use a tagger (Detailed documentation):</p> <p><pre><code>dolma tag \\\n    --documents \"/work/github/test_on_dagw_wiki/documents/dagw_only_wiki.json.gz\" \\\n    --experiment char_length_v1 \\\n    --taggers char_length_v1 \\\n    --processes 1\n</code></pre> Remark: <code>pii_presidio_v1</code> has a maximum text of length 1000000:  <pre><code>dolma.core.errors.DolmaFatalError: Failed to process /work/github/test_on_dagw_wiki/documents/dagw_only_wiki.json.gz due to ValueError: [E088] Text of length 1215638 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.\n</code></pre></p> # Dolma Tagger Description Process Time (In total, Speed) 1 char_length_v1 Computes document length in characters 16s, 16.2kd/s 2 char_length_with_paragraphs_v1 Computes document and paragraph length in characters 49s, 5.40kd/s 3 cld2_en_doc_v2 Detects document language using cld2 56s, 4.76kd/s 4 olmo_pretokenizer_v1 Counts number of tokens using OLMo v1 pre-tokenizer 6m57s, 645d/s 5 olmo_pretokenizer_with_paragraphs_v1 Counts tokens in document and paragraphs using OLMo v1 pre-tokenizer 7m02s, 636d/s 6 whitespace_tokenizer_v1 Counts whitespace-separated tokens in document 1m00s, 4.47kd/s 7 whitespace_tokenizer_with_paragraphs_v1 Counts whitespace-separated tokens in document and paragraphs 1m39s, 2.70kd/s 8 random_number_v1 Assigns a random number to each document 17s, 15.6kd/s 9 ft_lang_id_en_doc_v2 Uses fastText to detect the language of the document 2m28s, 1.82kd/s 10 ft_lang_id_en_paragraph_v2 Uses fastText to detect the language of each paragraph 6m21s, 705d/s 11 ft_lang_id_en_paragraph_with_doc_score_v2 Uses fastText to detect the language of each paragraph and assigns a score based on the fraction of English paragraphs 6m16s, 715d/s 12 gopher_v1 Tags spans of documents matching\u00a0Deepmind's Gopher\u00a0removal rules 15m49s, 283d/s 13 c4_v1 Implements taggers used to generate the\u00a0C4\u00a0dataset 3m50s, 1.17kd/s 14 c4_v2 Faster implementation of the C4 taggers 2m08s, 2.10kd/s 15 pii_presidio_v1 Tags spans of documents that contain personally identifiable information (PII) using the\u00a0Presidio Analyzer\u00a0library way to slow: about 7s per document. However <code>analyzer_results</code> in pii.py defines the language as English if . See line 110 in here 16 pii_regex_v1 Tags spans of documents that contain personally identifiable information (PII) using a set of regular expressions 2m55s, 1.53kd/s 17 pii_regex_v2 Faster implementation of\u00a0<code>pii_regex_v1</code> 2m51s, 1.57kd/s 18 pii_regex_with_counts_v2 Tags spans of documents that contain personally identifiable information (PII) using a set of regular expressions. It also counts the number of matches for each regular expression 2m43s, 1.65kd/s 19 pii_regex_with_counts_fast_v2 Faster implementation of\u00a0<code>pii_regex_with_counts_v2</code> 1m01s, 4.36kd/s 20 cld2_scandi_doc Language Detection using cld2 1m11s, 3.79kd/s 21 cld2_scandi_paragraph Language Detection on paragraph level using cld2 5m59s, 748d/s 22 ft_lang_id_scandi_doc FastText Language Detection 3m14s, 1.38kd/s 23 ft_lang_id_scandi_paragraph FastText Language Detection on paragraph level 14m06s, 318d/s 24 cld2_scandi_paragraph_with_doc_score Language Detection on paragraph level with a total score using cld2 8m04s, 556d/s 25 ft_lang_id_scandi_paragraph_with_doc_score FastText Language Detection on paragraph level with a total score 14m37s, 306d/s 26 jigsaw_hatespeech_document_v2 Tags documents as containing hate speech or not using a FastText classifier trained on the\u00a0Jigsaw\u00a0hate speech dataset. 1m38s, 2.74kd/s 27 jigsaw_hatespeech_sentence_v2 Tags spans of documents as containing hate speech or not using a FastText classifier trained on the\u00a0Jigsaw\u00a0hate speech dataset. 9m45s, 460d/s 28 jigsaw_nsfw_document_v1 Tags documents as containing NSFW content or not using a FastText classifier trained on the\u00a0Jigsaw\u00a0NSFW dataset. 6m40s, 671d/s 29 jigsaw_nsfw_sentence_v2 Tags spans of documents as containing NSFW content or not using a FastText classifier trained on the\u00a0Jigsaw\u00a0NSFW dataset. 9m02s, 496d/s"},{"location":"tutorials/finetune/","title":"Finetuning Language Models","text":"<p>This notebook will allow you to try out finetuning of the <code>munin-7b-alpha</code> model or, indeed, any other generative model out there.</p> <p>We'll be finetuning the model on a Danish translated instruction tuning dataset, using the QLoRA method.</p> In\u00a0[\u00a0]: Copied! <pre># Uncomment to install packages (already done for you)\n# %pip install --upgrade --force-reinstall --no-cache-dir torch==2.1.1 triton --index-url https://download.pytorch.org/whl/cu121\n# %pip install \"unsloth[cu121_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git\"\n</pre> # Uncomment to install packages (already done for you) # %pip install --upgrade --force-reinstall --no-cache-dir torch==2.1.1 triton --index-url https://download.pytorch.org/whl/cu121 # %pip install \"unsloth[cu121_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git\" In\u00a0[\u00a0]: Copied! <pre># General packages\nimport torch\nimport getpass\n\n# For loading the finetuning datasets\nfrom datasets import load_dataset\n\n# For loading and finetuning the models\nfrom unsloth import FastLanguageModel\nfrom trl import SFTTrainer, setup_chat_format\nfrom transformers import TrainingArguments, AutoTokenizer, TextStreamer, GenerationConfig\n</pre> # General packages import torch import getpass  # For loading the finetuning datasets from datasets import load_dataset  # For loading and finetuning the models from unsloth import FastLanguageModel from trl import SFTTrainer, setup_chat_format from transformers import TrainingArguments, AutoTokenizer, TextStreamer, GenerationConfig <p>To allow finetuning gated models (like LLaMA-2) and to upload your finetuned models, you can put your Hugging Face token in the cell below.</p> <p>You can generate a token at https://hf.co/settings/tokens.</p> <p>If you don't want to supply a token then simply leave it blank!</p> In\u00a0[\u00a0]: Copied! <pre>HUGGING_FACE_TOKEN = getpass.getpass(\"Hugging Face Token: \")\nif not HUGGING_FACE_TOKEN:\n    print(\"Not using a Hugging Face token.\")\n    HUGGING_FACE_TOKEN = None\n</pre> HUGGING_FACE_TOKEN = getpass.getpass(\"Hugging Face Token: \") if not HUGGING_FACE_TOKEN:     print(\"Not using a Hugging Face token.\")     HUGGING_FACE_TOKEN = None In\u00a0[\u00a0]: Copied! <pre>RANDOM_SEED = 42\n\nMODEL_CONFIGURATION = dict(\n    model_name=\"danish-foundation-models/munin-7b-alpha\",\n    max_seq_length=2048,  \n    dtype=None,  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+ GPUs\n    load_in_4bit=True,  # Use 4bit quantisation to reduce memory usage. Quantises on the fly, so can take a while.\n    attn_implementation=\"flash_attention_2\"\n)\n\nPEFT_CONFIGURATION = dict(\n    r = 16,  # Adapter rank, choose any number &gt; 0, but suggested 8, 16, 32, 64, 128\n    target_modules=[\n        \"q_proj\", \n        \"k_proj\", \n        \"v_proj\", \n        \"o_proj\", \n        \"gate_proj\", \n        \"up_proj\", \n        \"down_proj\",\n    ],\n    lora_alpha = 16,\n    lora_dropout = 0,  # Supports any, but = 0 is optimized\n    bias = \"none\",  # Supports any, but = \"none\" is optimized\n    use_gradient_checkpointing = True,\n    use_rslora = False,  # Support rank stabilized LoRA\n    loftq_config = None,  # And LoftQ\n    random_state = RANDOM_SEED,\n)\n\nFINETUNING_CONFIGURATION = dict(\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=1,\n    warmup_steps=5,\n    num_train_epochs=1,\n    learning_rate=2e-4,\n    weight_decay=0.01,\n    lr_scheduler_type=\"linear\",\n)\n</pre> RANDOM_SEED = 42  MODEL_CONFIGURATION = dict(     model_name=\"danish-foundation-models/munin-7b-alpha\",     max_seq_length=2048,       dtype=None,  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+ GPUs     load_in_4bit=True,  # Use 4bit quantisation to reduce memory usage. Quantises on the fly, so can take a while.     attn_implementation=\"flash_attention_2\" )  PEFT_CONFIGURATION = dict(     r = 16,  # Adapter rank, choose any number &gt; 0, but suggested 8, 16, 32, 64, 128     target_modules=[         \"q_proj\",          \"k_proj\",          \"v_proj\",          \"o_proj\",          \"gate_proj\",          \"up_proj\",          \"down_proj\",     ],     lora_alpha = 16,     lora_dropout = 0,  # Supports any, but = 0 is optimized     bias = \"none\",  # Supports any, but = \"none\" is optimized     use_gradient_checkpointing = True,     use_rslora = False,  # Support rank stabilized LoRA     loftq_config = None,  # And LoftQ     random_state = RANDOM_SEED, )  FINETUNING_CONFIGURATION = dict(     per_device_train_batch_size=8,     gradient_accumulation_steps=1,     warmup_steps=5,     num_train_epochs=1,     learning_rate=2e-4,     weight_decay=0.01,     lr_scheduler_type=\"linear\", ) In\u00a0[\u00a0]: Copied! <pre>model, tokenizer = FastLanguageModel.from_pretrained(**MODEL_CONFIGURATION, token=HUGGING_FACE_TOKEN)\nmodel, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\nmodel = FastLanguageModel.get_peft_model(model, **PEFT_CONFIGURATION)\n</pre> model, tokenizer = FastLanguageModel.from_pretrained(**MODEL_CONFIGURATION, token=HUGGING_FACE_TOKEN) model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer) model = FastLanguageModel.get_peft_model(model, **PEFT_CONFIGURATION) <p>Load the dataset from Hugging Face Hub:</p> In\u00a0[\u00a0]: Copied! <pre>dataset = load_dataset(\"kobprof/skolegpt-instruct\", split=\"train\")\nprint(f\"Number of samples in dataset: {len(dataset):,}\")\n</pre> dataset = load_dataset(\"kobprof/skolegpt-instruct\", split=\"train\") print(f\"Number of samples in dataset: {len(dataset):,}\") <p>We just take a random subset, 1000 samples should take around 7 minutes on this machine depending on settings.</p> In\u00a0[\u00a0]: Copied! <pre>n_samples = 1000\ndataset = dataset.shuffle(seed=RANDOM_SEED).select(range(n_samples))\n</pre> n_samples = 1000 dataset = dataset.shuffle(seed=RANDOM_SEED).select(range(n_samples)) <p>Lastly, we set up the conversations in the dataset into the standard ChatML format.</p> In\u00a0[\u00a0]: Copied! <pre>def create_conversation(sample: dict) -&gt; dict[str, list[dict[str, str]]]:\n    \"\"\"This converts the sample to the standardised ChatML format.\n\n    Args:\n        sample:\n            The data sample.\n\n    Returns:\n        The sample set up in the ChatML format.\n    \"\"\"\n    return {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": sample[\"system_prompt\"]},\n            {\"role\": \"user\", \"content\": sample[\"question\"]},\n            {\"role\": \"assistant\", \"content\": sample[\"response\"]}\n        ]\n    }\n\ndataset = dataset.map(create_conversation, batched=False)\n</pre> def create_conversation(sample: dict) -&gt; dict[str, list[dict[str, str]]]:     \"\"\"This converts the sample to the standardised ChatML format.      Args:         sample:             The data sample.      Returns:         The sample set up in the ChatML format.     \"\"\"     return {         \"messages\": [             {\"role\": \"system\", \"content\": sample[\"system_prompt\"]},             {\"role\": \"user\", \"content\": sample[\"question\"]},             {\"role\": \"assistant\", \"content\": sample[\"response\"]}         ]     }  dataset = dataset.map(create_conversation, batched=False) In\u00a0[\u00a0]: Copied! <pre>trainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    max_seq_length=MODEL_CONFIGURATION[\"max_seq_length\"],\n    dataset_num_proc=4,\n    packing=True,  # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        optim=\"adamw_8bit\",\n        fp16=not torch.cuda.is_bf16_supported(),\n        bf16=torch.cuda.is_bf16_supported(),\n        logging_steps=3,\n        seed=RANDOM_SEED,\n        output_dir=\"outputs\",\n        **FINETUNING_CONFIGURATION\n    ),\n)\n</pre> trainer = SFTTrainer(     model=model,     tokenizer=tokenizer,     train_dataset=dataset,     max_seq_length=MODEL_CONFIGURATION[\"max_seq_length\"],     dataset_num_proc=4,     packing=True,  # Can make training 5x faster for short sequences.     args = TrainingArguments(         optim=\"adamw_8bit\",         fp16=not torch.cuda.is_bf16_supported(),         bf16=torch.cuda.is_bf16_supported(),         logging_steps=3,         seed=RANDOM_SEED,         output_dir=\"outputs\",         **FINETUNING_CONFIGURATION     ), ) In\u00a0[\u00a0]: Copied! <pre># Log some GPU stats before we start the finetuning\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(\n    f\"You're using the {gpu_stats.name} GPU, which has {max_memory:.2f} GB of memory \"\n    f\"in total, of which {start_gpu_memory:.2f}GB has been reserved already.\"\n)\n</pre> # Log some GPU stats before we start the finetuning gpu_stats = torch.cuda.get_device_properties(0) start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3) max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3) print(     f\"You're using the {gpu_stats.name} GPU, which has {max_memory:.2f} GB of memory \"     f\"in total, of which {start_gpu_memory:.2f}GB has been reserved already.\" ) In\u00a0[\u00a0]: Copied! <pre># This is where the actual finetuning is happening\ntrainer_stats = trainer.train()\n</pre> # This is where the actual finetuning is happening trainer_stats = trainer.train() In\u00a0[\u00a0]: Copied! <pre># Log some post-training GPU statistics\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory / max_memory * 100, 3)\nlora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\nprint(\n    f\"We ended up using {used_memory:.2f} GB GPU memory ({used_percentage:.2f}%), \"\n    f\"of which {used_memory_for_lora:.2f} GB ({lora_percentage:.2f}%) \"\n    \"was used for LoRa.\"\n)\n</pre> # Log some post-training GPU statistics used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3) used_memory_for_lora = round(used_memory - start_gpu_memory, 3) used_percentage = round(used_memory / max_memory * 100, 3) lora_percentage = round(used_memory_for_lora / max_memory * 100, 3) print(     f\"We ended up using {used_memory:.2f} GB GPU memory ({used_percentage:.2f}%), \"     f\"of which {used_memory_for_lora:.2f} GB ({lora_percentage:.2f}%) \"     \"was used for LoRa.\" ) <p>Time to try out the new finetuned model. First we need to set up how to generate text with it.</p> <p>You can leave the following config as-is, or you can experiment. Here is a list of all the different arguments.</p> In\u00a0[\u00a0]: Copied! <pre>GENERATION_CONFIG = GenerationConfig(\n    # What should be outputted\n    max_new_tokens=256, \n\n    # Controlling how the model chooses the next token to generate\n    do_sample=True, \n    temperature=0.2, \n    repetition_penalty=1.2,\n    top_k=50,\n    top_p=0.95,\n\n    #\u00a0Miscellaneous required settings\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.eos_token_id,\n    use_cache=False,  #\u00a0Required by unsloth\n)\n</pre> GENERATION_CONFIG = GenerationConfig(     # What should be outputted     max_new_tokens=256,       # Controlling how the model chooses the next token to generate     do_sample=True,      temperature=0.2,      repetition_penalty=1.2,     top_k=50,     top_p=0.95,      #\u00a0Miscellaneous required settings     eos_token_id=tokenizer.eos_token_id,     pad_token_id=tokenizer.eos_token_id,     use_cache=False,  #\u00a0Required by unsloth ) <p>Let's use <code>TextStreamer</code> for continuous inference - so you can see the generation token by token, instead of waiting the whole time!</p> In\u00a0[\u00a0]: Copied! <pre>messages = [\n    dict(\n        role=\"system\",\n        content=\"\"  # Change this to anything you want\n    ),\n    dict(\n        role=\"user\",\n        content=\"Hvad synes du om Danish Foundation Models projektet? Skriv kortfattet.\"  # And change this too\n    ),\n]\n\noutputs = model.generate(\n    input_ids=tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\"),\n    streamer=TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True),\n    generation_config=GENERATION_CONFIG,\n)\n</pre> messages = [     dict(         role=\"system\",         content=\"\"  # Change this to anything you want     ),     dict(         role=\"user\",         content=\"Hvad synes du om Danish Foundation Models projektet? Skriv kortfattet.\"  # And change this too     ), ]  outputs = model.generate(     input_ids=tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\"),     streamer=TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True),     generation_config=GENERATION_CONFIG, ) <p>You can share your new model to the Hugging Face Hub - this requires that you've included your Hugging Face token at the top of this notebook.</p> In\u00a0[\u00a0]: Copied! <pre># model.push_to_hub(\"your_name/qlora_model\", token=HUGGING_FACE_TOKEN)\n</pre> # model.push_to_hub(\"your_name/qlora_model\", token=HUGGING_FACE_TOKEN) <p>The popular inference framework vLLM can take advantage of having a model available in lower precision, enabling faster inference times.</p> <p>You can uncomment the following lines if you want to save the model in 16-bit or even 4-bit precision:</p> In\u00a0[\u00a0]: Copied! <pre># Merge to 16bit\n# model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\",)\n# model.push_to_hub_merged(\"hf/model\", tokenizer, save_method=\"merged_16bit\", token=HUGGING_FACE_TOKEN)\n\n# Merge to 4bit\n# model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_4bit\",)\n# model.push_to_hub_merged(\"hf/model\", tokenizer, save_method=\"merged_4bit\", token=HUGGING_FACE_TOKEN)\n</pre> # Merge to 16bit # model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\",) # model.push_to_hub_merged(\"hf/model\", tokenizer, save_method=\"merged_16bit\", token=HUGGING_FACE_TOKEN)  # Merge to 4bit # model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_4bit\",) # model.push_to_hub_merged(\"hf/model\", tokenizer, save_method=\"merged_4bit\", token=HUGGING_FACE_TOKEN) <p>Alternatively, you can save only the adapter weights, which are very light, but which requires the base model to be able to use it:</p> In\u00a0[\u00a0]: Copied! <pre># Just LoRA adapters\n# model.save_pretrained_merged(\"model\", tokenizer, save_method=\"lora\",)\n# model.push_to_hub_merged(\"hf/model\", tokenizer, save_method=\"lora\", token=HUGGING_FACE_TOKEN)\n</pre> # Just LoRA adapters # model.save_pretrained_merged(\"model\", tokenizer, save_method=\"lora\",) # model.push_to_hub_merged(\"hf/model\", tokenizer, save_method=\"lora\", token=HUGGING_FACE_TOKEN) <p>You can also save the model in the popular <code>GGUF</code> or <code>llama.cpp</code> formats, by uncommenting any of the following:</p> In\u00a0[\u00a0]: Copied! <pre># Save to 8bit Q8_0\n# model.save_pretrained_gguf(\"model\", tokenizer)\n# model.push_to_hub_gguf(\"hf/model\", tokenizer, token=HUGGING_FACE_TOKEN)\n\n# Save to 16bit GGUF\n# model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"f16\")\n# model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method=\"f16\", token=HUGGING_FACE_TOKEN)\n\n# Save to q4_k_m GGUF\n# model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"q4_k_m\")\n# model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method=\"q4_k_m\", token=HUGGING_FACE_TOKEN)\n</pre> # Save to 8bit Q8_0 # model.save_pretrained_gguf(\"model\", tokenizer) # model.push_to_hub_gguf(\"hf/model\", tokenizer, token=HUGGING_FACE_TOKEN)  # Save to 16bit GGUF # model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"f16\") # model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method=\"f16\", token=HUGGING_FACE_TOKEN)  # Save to q4_k_m GGUF # model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"q4_k_m\") # model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method=\"q4_k_m\", token=HUGGING_FACE_TOKEN) <p>Now, use the <code>model-unsloth.gguf</code> file or <code>model-unsloth-Q4_K_M.gguf</code> file in <code>llama.cpp</code> or a UI based system like <code>GPT4All</code>. You can install GPT4All by going here.</p>"},{"location":"tutorials/finetune/#draft-false-date-2024-02-02","title":"draft: false date: 2024-02-02\u00b6","text":""},{"location":"tutorials/finetune/#finetuning-language-models","title":"Finetuning Language Models\u00b6","text":""},{"location":"tutorials/finetune/#install-dependencies","title":"Install Dependencies\u00b6","text":""},{"location":"tutorials/finetune/#get-hugging-face-token","title":"Get Hugging Face Token\u00b6","text":""},{"location":"tutorials/finetune/#configure-the-model","title":"Configure the Model\u00b6","text":""},{"location":"tutorials/finetune/#load-the-model","title":"Load the Model\u00b6","text":""},{"location":"tutorials/finetune/#load-and-prepare-data","title":"Load and Prepare Data\u00b6","text":""},{"location":"tutorials/finetune/#finetune","title":"Finetune!\u00b6","text":""},{"location":"tutorials/finetune/#try-it-out","title":"Try it Out\u00b6","text":""},{"location":"tutorials/finetune/#share-the-model","title":"Share the Model\u00b6","text":""},{"location":"tutorials/finetune/#extra-export-model-to-other-frameworks","title":"Extra: Export Model to Other Frameworks\u00b6","text":""},{"location":"tutorials/finetune/#saving-to-float16-for-vllm","title":"Saving to float16 for vLLM\u00b6","text":""},{"location":"tutorials/finetune/#gguf-llamacpp-conversion","title":"GGUF / llama.cpp Conversion\u00b6","text":""},{"location":"tutorials/merge/","title":"Merging Language Models","text":"<p>Model merging is a relatively new method that allows one to combine the weights of different language models into a single model.</p> <p>In this notebook you'll get to try this out, as well as try to interact with the merged model to see the results!</p> <p>The mergekit README is good to have open for this notebook. It has descriptions and examples for the different merge methods it supports.</p> In\u00a0[\u00a0]: Copied! <pre># Uncomment to install packages (already done for you)\n# !git clone https://github.com/cg123/mergekit.git\n# %cd mergekit\n# %pip install -e .\n# %cd ..\n</pre> # Uncomment to install packages (already done for you) # !git clone https://github.com/cg123/mergekit.git # %cd mergekit # %pip install -e . # %cd .. In\u00a0[\u00a0]: Copied! <pre># General packages\nimport torch\nimport shutil\nfrom pathlib import Path\n\n# For merging the models\nfrom mergekit.config import MergeConfiguration\nfrom mergekit.merge import MergeOptions, run_merge\n\n# For loading the models and running them after the merge\nimport transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, GenerationConfig\n</pre> # General packages import torch import shutil from pathlib import Path  # For merging the models from mergekit.config import MergeConfiguration from mergekit.merge import MergeOptions, run_merge  # For loading the models and running them after the merge import transformers from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, GenerationConfig <p>To allow merging gated models (like LLaMA-2) and to upload your merged models, you can put your Hugging Face token in the cell below.</p> <p>You can generate a token at https://hf.co/settings/tokens.</p> <p>If you don't want to supply a token then simply leave it blank!</p> In\u00a0[\u00a0]: Copied! <pre>import getpass\nHUGGING_FACE_TOKEN = getpass.getpass(\"Hugging Face Token: \")\nif not HUGGING_FACE_TOKEN:\n    print(\"Not using a Hugging Face token.\")\n    HUGGING_FACE_TOKEN = None\n</pre> import getpass HUGGING_FACE_TOKEN = getpass.getpass(\"Hugging Face Token: \") if not HUGGING_FACE_TOKEN:     print(\"Not using a Hugging Face token.\")     HUGGING_FACE_TOKEN = None <p>This is where we set up which models we would like to merge, and which merging method to use.</p> <p>This configuration was the configuration used to create the Munin-NeuralBeagle model, but you can change it to whatever you like!</p> In\u00a0[\u00a0]: Copied! <pre>merge_config = dict(\n    models=[\n        dict(\n            model=\"danish-foundation-models/munin-7b-alpha\",\n        ),\n        dict(\n            model=\"mlabonne/NeuralBeagle14-7B\",\n            parameters=dict(\n                density=0.53,\n                weight=0.6,\n            ),\n        ),\n    ],\n    merge_method=\"dare_ties\",\n    base_model=\"danish-foundation-models/munin-7b-alpha\",\n    parameters=dict(\n        int8_mask=True,\n    ),\n    dtype=\"bfloat16\",\n)\n</pre> merge_config = dict(     models=[         dict(             model=\"danish-foundation-models/munin-7b-alpha\",         ),         dict(             model=\"mlabonne/NeuralBeagle14-7B\",             parameters=dict(                 density=0.53,                 weight=0.6,             ),         ),     ],     merge_method=\"dare_ties\",     base_model=\"danish-foundation-models/munin-7b-alpha\",     parameters=dict(         int8_mask=True,     ),     dtype=\"bfloat16\", ) In\u00a0[\u00a0]: Copied! <pre>LAZY_UNPICKLE = False  # Experimental low-memory model loader\nLOW_CPU_MEMORY = True  # Enable if you have more VRAM than RAM+swap\nOUT_PATH = \"./merged\"\n</pre> LAZY_UNPICKLE = False  # Experimental low-memory model loader LOW_CPU_MEMORY = True  # Enable if you have more VRAM than RAM+swap OUT_PATH = \"./merged\" In\u00a0[\u00a0]: Copied! <pre>run_merge(\n    MergeConfiguration.model_validate(merge_config),\n    out_path=OUT_PATH,\n    options=MergeOptions(\n        lora_merge_cache=\"/tmp\",\n        cuda=torch.cuda.is_available(),\n        copy_tokenizer=True,\n        lazy_unpickle=LAZY_UNPICKLE,\n        low_cpu_memory=LOW_CPU_MEMORY,\n    )\n)\n</pre> run_merge(     MergeConfiguration.model_validate(merge_config),     out_path=OUT_PATH,     options=MergeOptions(         lora_merge_cache=\"/tmp\",         cuda=torch.cuda.is_available(),         copy_tokenizer=True,         lazy_unpickle=LAZY_UNPICKLE,         low_cpu_memory=LOW_CPU_MEMORY,     ) ) <p>Time to try out the new merged model. Let's start by loading it from disk.</p> In\u00a0[\u00a0]: Copied! <pre>model = AutoModelForCausalLM.from_pretrained(OUT_PATH, load_in_4bit=True)\ntokenizer = AutoTokenizer.from_pretrained(OUT_PATH)\n\n# Choosing a chat template for a merged model can be difficult. The one defined in \n# NeuralBeagle seems broken. Additionally, it does not have special tokens that some \n# of the merged models might have been trained with\ntokenizer.chat_template = \"\"\"\n{% if not add_generation_prompt is defined %}\n    {% set add_generation_prompt = false %}\n{% endif %}\n{% for message in messages %}\n    {{'&lt;|im_start|&gt;' + message['role'] + '\\n' + message['content'] + '&lt;|im_end|&gt;' + '\\n'}}\n{% endfor %}\n{% if add_generation_prompt %}\n    {{ '&lt;|im_start|&gt;assistant\\n' }}\n{% endif %}\n\"\"\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    device_map=\"auto\",\n)\n</pre> model = AutoModelForCausalLM.from_pretrained(OUT_PATH, load_in_4bit=True) tokenizer = AutoTokenizer.from_pretrained(OUT_PATH)  # Choosing a chat template for a merged model can be difficult. The one defined in  # NeuralBeagle seems broken. Additionally, it does not have special tokens that some  # of the merged models might have been trained with tokenizer.chat_template = \"\"\" {% if not add_generation_prompt is defined %}     {% set add_generation_prompt = false %} {% endif %} {% for message in messages %}     {{'&lt;|im_start|&gt;' + message['role'] + '\\n' + message['content'] + '&lt;|im_end|&gt;' + '\\n'}} {% endfor %} {% if add_generation_prompt %}     {{ '&lt;|im_start|&gt;assistant\\n' }} {% endif %} \"\"\"  pipeline = transformers.pipeline(     \"text-generation\",     model=model,     tokenizer=tokenizer,     device_map=\"auto\", ) <p>Next, we need to set up how to generate text with it. You can leave the following config as-is, or you can experiment. Here is a list of all the different arguments.</p> In\u00a0[\u00a0]: Copied! <pre>GENERATION_CONFIG = GenerationConfig(\n    # What should be outputted\n    max_new_tokens=256, \n\n    # Controlling how the model chooses the next token to generate\n    do_sample=True, \n    temperature=0.2, \n    repetition_penalty=1.2,\n    top_k=50,\n    top_p=0.95,\n\n    #\u00a0Miscellaneous required settings\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.eos_token_id\n)\n</pre> GENERATION_CONFIG = GenerationConfig(     # What should be outputted     max_new_tokens=256,       # Controlling how the model chooses the next token to generate     do_sample=True,      temperature=0.2,      repetition_penalty=1.2,     top_k=50,     top_p=0.95,      #\u00a0Miscellaneous required settings     eos_token_id=tokenizer.eos_token_id,     pad_token_id=tokenizer.eos_token_id ) In\u00a0[\u00a0]: Copied! <pre>messages = [\n    dict(\n        role=\"system\",\n        content=\"\"  # Change this to anything you want\n    ),\n    dict(\n        role=\"user\",\n        content=\"Hvad er en stor sprogmodel?\"  # And change this too\n    ),\n]\n\noutputs = pipeline(\n    tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True), \n    streamer=TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True),\n    generation_config=GENERATION_CONFIG,\n)\n</pre> messages = [     dict(         role=\"system\",         content=\"\"  # Change this to anything you want     ),     dict(         role=\"user\",         content=\"Hvad er en stor sprogmodel?\"  # And change this too     ), ]  outputs = pipeline(     tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True),      streamer=TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True),     generation_config=GENERATION_CONFIG, ) <p>You can share your new model to the Hugging Face Hub - this requires that you've included your Hugging Face token at the top of this notebook.</p> In\u00a0[\u00a0]: Copied! <pre># model.push_to_hub(\"your_name/merged_model\", token=HUGGING_FACE_TOKEN)\n</pre> # model.push_to_hub(\"your_name/merged_model\", token=HUGGING_FACE_TOKEN) <p>This deletes the merged model, as well as clearing the Hugging Face cache.</p> <p>WARNING: You will have to redownload any used models if you do this!</p> In\u00a0[\u00a0]: Copied! <pre># shutil.rmtree(OUT_PATH, ignore_errors=True)\n# shutil.rmtree('/home/ubuntu/.cache', ignore_errors=True)\n</pre> # shutil.rmtree(OUT_PATH, ignore_errors=True) # shutil.rmtree('/home/ubuntu/.cache', ignore_errors=True)"},{"location":"tutorials/merge/#draft-false-date-2024-02-02","title":"draft: false date: 2024-02-02\u00b6","text":""},{"location":"tutorials/merge/#merging-language-models","title":"Merging Language Models\u00b6","text":""},{"location":"tutorials/merge/#install-dependencies","title":"Install Dependencies\u00b6","text":""},{"location":"tutorials/merge/#get-hugging-face-token","title":"Get Hugging Face Token\u00b6","text":""},{"location":"tutorials/merge/#configure-the-merge","title":"Configure the Merge\u00b6","text":""},{"location":"tutorials/merge/#merge","title":"Merge!\u00b6","text":""},{"location":"tutorials/merge/#try-it-out","title":"Try it Out\u00b6","text":""},{"location":"tutorials/merge/#share-the-model","title":"Share the Model\u00b6","text":""},{"location":"tutorials/merge/#clean-up","title":"Clean Up\u00b6","text":""},{"location":"blog/archive/2024/","title":"2024","text":""}]}